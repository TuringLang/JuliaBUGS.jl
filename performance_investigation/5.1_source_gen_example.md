This is an example of the source generation approach using `Rats` model.

The original definition of the model is:

```julia
quote
    for i = 1:N
        for j = 1:T
            Y[i, j] ~ dnorm(mu[i, j], var"tau.c")
            mu[i, j] = alpha[i] + beta[i] * (x[j] - xbar)
        end
        alpha[i] ~ dnorm(var"alpha.c", var"alpha.tau")
        beta[i] ~ dnorm(var"beta.c", var"beta.tau")
    end
    var"tau.c" ~ dgamma(0.001, 0.001)
    sigma = 1 / sqrt(var"tau.c")
    var"alpha.c" ~ dnorm(0.0, 1.0e-6)
    var"alpha.tau" ~ dgamma(0.001, 0.001)
    var"beta.c" ~ dnorm(0.0, 1.0e-6)
    var"beta.tau" ~ dgamma(0.001, 0.001)
    alpha0 = var"alpha.c" - xbar * var"beta.c"
end
```

The sequential-ized fissioned version of the model is:

```julia
quote
    var"beta.tau" ~ dgamma(0.001, 0.001)
    var"beta.c" ~ dnorm(0.0, 1.0e-6)
    var"alpha.tau" ~ dgamma(0.001, 0.001)
    var"alpha.c" ~ dnorm(0.0, 1.0e-6)
    alpha0 = var"alpha.c" - xbar * var"beta.c"
    var"tau.c" ~ dgamma(0.001, 0.001)
    sigma = 1 / sqrt(var"tau.c")
    for i = 1:30
        beta[i] ~ dnorm(var"beta.c", var"beta.tau")
    end
    for i = 1:30
        alpha[i] ~ dnorm(var"alpha.c", var"alpha.tau")
    end
    for i = 1:30
        for j = 1:5
            mu[i, j] = alpha[i] + beta[i] * (x[j] - xbar)
        end
    end
    for i = 1:30
        for j = 1:5
            Y[i, j] ~ dnorm(mu[i, j], var"tau.c")
        end
    end
end
```

To compute the log-probability, the above AST is transformed into the following function:

```julia
function __logp__(__evaluation_env__, __flattened_values__)
    (; alpha, var"beta.c", xbar, sigma, alpha0, x, N, var"alpha.c", mu, 
       var"alpha.tau", Y, T, beta, var"beta.tau", var"tau.c") = __evaluation_env__
    
    __logp__ = 0.0

    # beta.tau
    __dist__ = dgamma(0.001, 0.001)
    __b__ = Bijectors.bijector(__dist__)
    __b_inv__ = Bijectors.inverse(__b__)
    __reconstructed_value__ = JuliaBUGS.reconstruct(__b_inv__, __dist__, view(__flattened_values__, 1:1))
    (__value__, __logjac__) = Bijectors.with_logabsdet_jacobian(__b_inv__, __reconstructed_value__)
    __logprior__ = Distributions.logpdf(__dist__, __value__) + __logjac__
    __logp__ = __logp__ + __logprior__
    var"beta.tau" = __value__

    # beta.c
    __dist__ = dnorm(0.0, 1.0e-6)
    __b__ = Bijectors.bijector(__dist__)
    __b_inv__ = Bijectors.inverse(__b__)
    __reconstructed_value__ = JuliaBUGS.reconstruct(__b_inv__, __dist__, view(__flattened_values__, 2:2))
    (__value__, __logjac__) = Bijectors.with_logabsdet_jacobian(__b_inv__, __reconstructed_value__)
    __logprior__ = Distributions.logpdf(__dist__, __value__) + __logjac__
    __logp__ = __logp__ + __logprior__
    var"beta.c" = __value__

    # alpha.tau
    __dist__ = dgamma(0.001, 0.001)
    __b__ = Bijectors.bijector(__dist__)
    __b_inv__ = Bijectors.inverse(__b__)
    __reconstructed_value__ = JuliaBUGS.reconstruct(__b_inv__, __dist__, view(__flattened_values__, 3:3))
    (__value__, __logjac__) = Bijectors.with_logabsdet_jacobian(__b_inv__, __reconstructed_value__)
    __logprior__ = Distributions.logpdf(__dist__, __value__) + __logjac__
    __logp__ = __logp__ + __logprior__
    var"alpha.tau" = __value__

    # alpha.c
    __dist__ = dnorm(0.0, 1.0e-6)
    __b__ = Bijectors.bijector(__dist__)
    __b_inv__ = Bijectors.inverse(__b__)
    __reconstructed_value__ = JuliaBUGS.reconstruct(__b_inv__, __dist__, view(__flattened_values__, 4:4))
    (__value__, __logjac__) = Bijectors.with_logabsdet_jacobian(__b_inv__, __reconstructed_value__)
    __logprior__ = Distributions.logpdf(__dist__, __value__) + __logjac__
    __logp__ = __logp__ + __logprior__
    var"alpha.c" = __value__

    alpha0 = var"alpha.c" - xbar * var"beta.c"

    # tau.c
    __dist__ = dgamma(0.001, 0.001)
    __b__ = Bijectors.bijector(__dist__)
    __b_inv__ = Bijectors.inverse(__b__)
    __reconstructed_value__ = JuliaBUGS.reconstruct(__b_inv__, __dist__, view(__flattened_values__, 5:5))
    (__value__, __logjac__) = Bijectors.with_logabsdet_jacobian(__b_inv__, __reconstructed_value__)
    __logprior__ = Distributions.logpdf(__dist__, __value__) + __logjac__
    __logp__ = __logp__ + __logprior__
    var"tau.c" = __value__

    sigma = 1 / sqrt(var"tau.c")

    # beta[i]
    for i = 1:30
        __dist__ = dnorm(var"beta.c", var"beta.tau")
        __b__ = Bijectors.bijector(__dist__)
        __b_inv__ = Bijectors.inverse(__b__)
        __reconstructed_value__ = JuliaBUGS.reconstruct(__b_inv__, __dist__, 
            view(__flattened_values__, 6 + (i - 1) * 1:6 + (i - 1) * 1))
        (__value__, __logjac__) = Bijectors.with_logabsdet_jacobian(__b_inv__, __reconstructed_value__)
        __logprior__ = Distributions.logpdf(__dist__, __value__) + __logjac__
        __logp__ = __logp__ + __logprior__
        beta[i] = __value__
    end

    # alpha[i]
    for i = 1:30
        __dist__ = dnorm(var"alpha.c", var"alpha.tau")
        __b__ = Bijectors.bijector(__dist__)
        __b_inv__ = Bijectors.inverse(__b__)
        __reconstructed_value__ = JuliaBUGS.reconstruct(__b_inv__, __dist__, 
            view(__flattened_values__, 36 + (i - 1) * 1:36 + (i - 1) * 1))
        (__value__, __logjac__) = Bijectors.with_logabsdet_jacobian(__b_inv__, __reconstructed_value__)
        __logprior__ = Distributions.logpdf(__dist__, __value__) + __logjac__
        __logp__ = __logp__ + __logprior__
        alpha[i] = __value__
    end

    # mu[i,j]
    for i = 1:30
        for j = 1:5
            mu[i, j] = alpha[i] + beta[i] * (x[j] - xbar)
        end
    end

    # Y[i,j]
    for i = 1:30
        for j = 1:5
            __logp__ += logpdf(dnorm(mu[i, j], var"tau.c"), Y[i, j])
        end
    end

    return __logp__
end
```

```julia
using Pkg
Pkg.activate(; temp=true)
Pkg.add(["JuliaBUGS", "Bijectors", "Distributions", "DifferentiationInterface", "Mooncake", "BenchmarkTools"])

using JuliaBUGS, Bijectors, Distributions
using JuliaBUGS: dnorm, dgamma

_evaluation_env = (
    alpha = fill(0.9, 30),
    var"beta.c" = 10.0,
    xbar = 22,
    sigma = 1.0,
    alpha0 = -70.0,
    x = [8.0, 15.0, 22.0, 29.0, 36.0],
    N = 30,
    var"alpha.c" = 150.0,
    mu = [-11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5; -11.7 -5.3999999999999995 0.9 7.2 13.5],
    var"alpha.tau" = 1.0,
    Y = [
        151 199 246 283 320
        145 199 249 293 354
        147 214 263 312 328
        155 200 237 272 297
        135 188 230 280 323
        159 210 252 298 331
        141 189 231 275 305
        159 201 248 297 338
        177 236 285 350 376
        134 182 220 260 296
        160 208 261 313 352
        143 188 220 273 314
        154 200 244 289 325
        171 221 270 326 358
        163 216 242 281 312
        160 207 248 288 324
        142 187 234 280 316
        156 203 243 283 317
        157 212 259 307 336
        152 203 246 286 321
        154 205 253 298 334
        139 190 225 267 302
        146 191 229 272 302
        157 211 250 285 323
        132 185 237 286 331
        160 207 257 303 345
        169 216 261 295 333
        157 205 248 289 316
        137 180 219 258 291
        153 200 244 286 324
    ],
    T = 5,
    beta = fill(0.9, 30),
    var"beta.tau" = 1.0,
    var"tau.c" = 1.0
)

p = fill(0.9, 65)
__logp__(_evaluation_env, p)
logp = Base.Fix1(__logp__, _evaluation_env)

using DifferentiationInterface, Mooncake

backend = AutoMooncake(; config=Mooncake.Config())
prep = prepare_gradient(logp, backend, p) # this is taking a bit of time the first time I run it

using BenchmarkTools

@benchmark __logp__($_evaluation_env, $p)
@benchmark logp($p) # the closure is slightly slower (875 ns vs 828 ns on a particular run on my machine)
@benchmark gradient($logp, $prep, $backend, $p)
```


